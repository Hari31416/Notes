
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A collection of my notes as well as some external materials">
      
      
        <meta name="author" content="Harikesh Kushwaha">
      
      
        <link rel="canonical" href="https://hari31416.github.io/Notes/0.4/Notes/ML/Ensemble_Learning/">
      
      
        <link rel="prev" href="../Decision_Trees/">
      
      
        <link rel="next" href="../Feature_Selection/">
      
      <link rel="icon" href="../../../assets/ml.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.15">
    
    
      
        <title>Ensemble Learning - Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#Contents" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Notes" class="md-header__button md-logo" aria-label="Notes" data-md-component="logo">
      
  <img src="../../../assets/ml.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Ensemble Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/Hari31416/Notes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Notes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Notes" class="md-nav__button md-logo" aria-label="Notes" data-md-component="logo">
      
  <img src="../../../assets/ml.png" alt="logo">

    </a>
    Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Hari31416/Notes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Notes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          DL
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          DL
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/CNN/" class="md-nav__link">
        CNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/Creating_NN_From_Scratch/" class="md-nav__link">
        Creating NN From Scratch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/Different_Initializations/" class="md-nav__link">
        Different Initializations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/Neural_Networks/" class="md-nav__link">
        Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/Optimization_Methods/" class="md-nav__link">
        Optimization Methods
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/Sequence_Models/" class="md-nav__link">
        Sequence Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DL/temp/" class="md-nav__link">
        Temp
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          DSA
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          DSA
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Bitwise_operators/" class="md-nav__link">
        Bitwise operators
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Doubly_Linked_List/" class="md-nav__link">
        Doubly Linked List
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/LinkedList/" class="md-nav__link">
        LinkedList
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_GfG_Easy_1/" class="md-nav__link">
        Problems GfG Easy 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_GfG_Easy_2/" class="md-nav__link">
        Problems GfG Easy 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_GfG_Easy_3/" class="md-nav__link">
        Problems GfG Easy 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_GfG_Easy_4/" class="md-nav__link">
        Problems GfG Easy 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Easy_1/" class="md-nav__link">
        Problems Leetcode Easy 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Easy_2/" class="md-nav__link">
        Problems Leetcode Easy 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Easy_3/" class="md-nav__link">
        Problems Leetcode Easy 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Easy_4/" class="md-nav__link">
        Problems Leetcode Easy 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Easy_5/" class="md-nav__link">
        Problems Leetcode Easy 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Easy_6/" class="md-nav__link">
        Problems Leetcode Easy 6
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Hard_1/" class="md-nav__link">
        Problems Leetcode Hard 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Medium_1/" class="md-nav__link">
        Problems Leetcode Medium 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Medium_2/" class="md-nav__link">
        Problems Leetcode Medium 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Problems_Leetcode_Medium_3/" class="md-nav__link">
        Problems Leetcode Medium 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Recursion/" class="md-nav__link">
        Recursion
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Stack_and_Queue/" class="md-nav__link">
        Stack and Queue
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../DSA/Trees/" class="md-nav__link">
        Trees
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
          Finance
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Finance
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Black_Scholes_Model/" class="md-nav__link">
        Black Scholes Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/CAPM/" class="md-nav__link">
        CAPM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Five_Factor_Model/" class="md-nav__link">
        Five Factor Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Forward_Future/" class="md-nav__link">
        Forward Future
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Market_Hypothesis/" class="md-nav__link">
        Market Hypothesis
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_1/" class="md-nav__link">
        Module 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_2/" class="md-nav__link">
        Module 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_3/" class="md-nav__link">
        Module 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_4/" class="md-nav__link">
        Module 4
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_5/" class="md-nav__link">
        Module 5
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_6/" class="md-nav__link">
        Module 6
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Module_7/" class="md-nav__link">
        Module 7
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Finance/Working_With_pystock/" class="md-nav__link">
        Working With pystock
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
          ML
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2_4">
          <span class="md-nav__icon md-icon"></span>
          ML
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Clustering/" class="md-nav__link">
        Clustering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Decision_Trees/" class="md-nav__link">
        Decision Trees
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Ensemble Learning
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Feature_Selection/" class="md-nav__link">
        Feature Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Linear_Regression/" class="md-nav__link">
        Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Logistic_Regression/" class="md-nav__link">
        Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Multiclass_Multioutput_Algorithm/" class="md-nav__link">
        Multiclass Multioutput Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Nearest_Neighbors/" class="md-nav__link">
        Nearest Neighbors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Stochastic_Gradient_Descent/" class="md-nav__link">
        Stochastic Gradient Descent
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Support_Vector_Machine/" class="md-nav__link">
        Support Vector Machine
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
      
      
      
        <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_5">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python/OOP/" class="md-nav__link">
        OOP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python/Programming_Language_Terms/" class="md-nav__link">
        Programming Language Terms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python/Programming_Paradigms/" class="md-nav__link">
        Programming Paradigms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python/Scope_and_Namespace/" class="md-nav__link">
        Scope and Namespace
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python/Terminology/" class="md-nav__link">
        Terminology
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python/Variables_and_Pointers/" class="md-nav__link">
        Variables and Pointers
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
      
      
      
        <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
          Numerical_Methods
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_6">
          <span class="md-nav__icon md-icon"></span>
          Numerical_Methods
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Basics_of_FDTD/" class="md-nav__link">
        Basics of FDTD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Curve_Fitting/" class="md-nav__link">
        Curve Fitting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Eigenvalue_Eigenvector/" class="md-nav__link">
        Eigenvalue Eigenvector
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/FD_and_Boris_Method/" class="md-nav__link">
        FD and Boris Method
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Interpolation/" class="md-nav__link">
        Interpolation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Numerical_Differentiation/" class="md-nav__link">
        Numerical Differentiation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Numerical_Integration/" class="md-nav__link">
        Numerical Integration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/ODE/" class="md-nav__link">
        ODE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Root_Finding/" class="md-nav__link">
        Root Finding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Single_Particle_Motions/" class="md-nav__link">
        Single Particle Motions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Solving_ODEs/" class="md-nav__link">
        Solving ODEs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Numerical_Methods/Some_ODEs/" class="md-nav__link">
        Some ODEs
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
          manim
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_7">
          <span class="md-nav__icon md-icon"></span>
          manim
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/3Dscenes/" class="md-nav__link">
        3Dscenes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/Animations/" class="md-nav__link">
        Animations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/Camera_Resolution/" class="md-nav__link">
        Camera Resolution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/manim_in_Jupyter/" class="md-nav__link">
        manim in Jupyter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/manim_output_settings/" class="md-nav__link">
        Manim output settings
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/Mobjects/" class="md-nav__link">
        Mobjects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../manim/Text_and_Formula/" class="md-nav__link">
        Text and Formula
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script>
(function() {
  function addWidgetsRenderer() {
    var requireJsScript = document.createElement('script');
    requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';

    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var jupyterWidgetsScript = document.createElement('script');
    var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    jupyterWidgetsScript.src = widgetRendererSrc;

    document.body.appendChild(requireJsScript);
    document.body.appendChild(jupyterWidgetsScript);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Contents">Contents<a href="#Contents"></a></h1>
<div class="highlight"><pre><span></span><code>    &lt;ol&gt;
    &lt;li&gt;&lt;a class=&quot;&quot; href=&quot;#Ensemble-Learning&quot;&gt;Ensemble Learning&lt;/a&gt;&lt;/li&gt;
</code></pre></div>
<li><a class="" href="#Average-Ensemble-Methods">Average Ensemble Methods</a></li>
<ol><li><a class="" href="#Bagging-meta-estimator">Bagging meta-estimator</a></li>
<li><a class="" href="#Forests-of-randomized-trees">Forests of randomized trees</a></li>
<ol><li><a class="" href="#Random-Forest">Random Forest</a></li>
<li><a class="" href="#Extremely-Randomized-Trees">Extremely Randomized Trees</a></li>
<li><a class="" href="#Parameters">Parameters</a></li>
<li><a class="" href="#Time-Complexity">Time Complexity</a></li>
<li><a class="" href="#Parallelization">Parallelization</a></li>
<li><a class="" href="#Feature-Importance">Feature Importance</a></li>
</ol><li><a class="" href="#Voting-Classifier">Voting Classifier</a></li>
<ol><li><a class="" href="#Majority-Class-Labels-(Hard-Voting)">Majority Class Labels (Hard Voting)</a></li>
<li><a class="" href="#Weighted-Average-Probabilities-(Soft-Voting)">Weighted Average Probabilities (Soft Voting)</a></li>
<li><a class="" href="#Using-the-VotingClassifier-with-GridSearchCV">Using the VotingClassifier with GridSearchCV</a></li>
</ol><li><a class="" href="#Voting-Regressor">Voting Regressor</a></li>
<li><a class="" href="#Stacked-generalization">Stacked generalization</a></li>
</ol>
<p><li><a class="" href="#Boosting-Methods">Boosting Methods</a></li></p>
<ol><li><a class="" href="#AdaBoost">AdaBoost</a></li>
<li><a class="" href="#Gradient-Tree-Boosting">Gradient Tree Boosting</a></li>
<ol><li><a class="" href="#Controlling-the-Tree-Size">Controlling the Tree Size</a></li>
<li><a class="" href="#Loss-Functions">Loss Functions</a></li>
</ol><li><a class="" href="#Histogram-Based-Gradient-Boosting">Histogram-Based Gradient Boosting</a></li>
<ol><li><a class="" href="#Missing-values-support">Missing values support</a></li>
<li><a class="" href="#Sample-weight-support">Sample weight support</a></li>
<li><a class="" href="#Monotonic-Constraints">Monotonic Constraints</a></li>
</ol><li><a class="" href="#Algorithms">Algorithms</a></li>
<ol><li><a class="" href="#BaggingClassifier">BaggingClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-BaggingClassifier">Parameters of the BaggingClassifier</a></li>
<li><a class="" href="#Attributes-of-the-BaggingClassifier">Attributes of the BaggingClassifier</a></li>
</ol><li><a class="" href="#BaggingRegressor">BaggingRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-BaggingRegressor">Parameters of the BaggingRegressor</a></li>
<li><a class="" href="#Attributes-of-the-BaggingRegressor">Attributes of the BaggingRegressor</a></li>
</ol><li><a class="" href="#RandomForestClassifier">RandomForestClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-RandomForestClassifier">Parameters of the RandomForestClassifier</a></li>
<li><a class="" href="#Attributes-of-the-RandomForestClassifier">Attributes of the RandomForestClassifier</a></li>
</ol><li><a class="" href="#RandomForestRegressor">RandomForestRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-RandomForestRegressor">Parameters of the RandomForestRegressor</a></li>
<li><a class="" href="#Attributes-of-the-RandomForestRegressor">Attributes of the RandomForestRegressor</a></li>
</ol><li><a class="" href="#ExtraTreesClassifier">ExtraTreesClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-ExtraTreesClassifier">Parameters of the ExtraTreesClassifier</a></li>
<li><a class="" href="#Attributes-of-the-ExtraTreesClassifier">Attributes of the ExtraTreesClassifier</a></li>
</ol><li><a class="" href="#ExtraTreesRegressor">ExtraTreesRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-ExtraTreesRegressor">Parameters of the ExtraTreesRegressor</a></li>
<li><a class="" href="#Attributes-of-the-ExtraTreesRegressor">Attributes of the ExtraTreesRegressor</a></li>
</ol><li><a class="" href="#VotingClassifier">VotingClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-VotingClassifier">Parameters of the VotingClassifier</a></li>
<li><a class="" href="#Attributes-of-the-VotingClassifier">Attributes of the VotingClassifier</a></li>
</ol><li><a class="" href="#VotingRegressor">VotingRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-VotingRegressor">Parameters of the VotingRegressor</a></li>
<li><a class="" href="#Attributes-of-the-VotingRegressor">Attributes of the VotingRegressor</a></li>
</ol><li><a class="" href="#StackingClassifier">StackingClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-StackingClassifier">Parameters of the StackingClassifier</a></li>
<li><a class="" href="#Attributes-of-the-StackingClassifier">Attributes of the StackingClassifier</a></li>
</ol><li><a class="" href="#StackingRegressor">StackingRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-StackingRegressor">Parameters of the StackingRegressor</a></li>
<li><a class="" href="#Attributes-of-the-StackingRegressor">Attributes of the StackingRegressor</a></li>
</ol><li><a class="" href="#AdaBoostClassifier">AdaBoostClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-AdaBoostClassifier">Parameters of the AdaBoostClassifier</a></li>
<li><a class="" href="#Attributes-of-the-AdaBoostClassifier">Attributes of the AdaBoostClassifier</a></li>
</ol><li><a class="" href="#AdaBoostRegressor">AdaBoostRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-AdaBoostRegressor">Parameters of the AdaBoostRegressor</a></li>
<li><a class="" href="#Attributes-of-the-AdaBoostRegressor">Attributes of the AdaBoostRegressor</a></li>
</ol><li><a class="" href="#GradientBoostingClassifier">GradientBoostingClassifier</a></li>
<ol><li><a class="" href="#Attributes-of-the-GradientBoostingClassifier">Attributes of the GradientBoostingClassifier</a></li>
</ol><li><a class="" href="#GradientBoostingRegressor">GradientBoostingRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-GradientBoostingRegressor">Parameters of the GradientBoostingRegressor</a></li>
<li><a class="" href="#Attributes-of-the-GradientBoostingRegressor">Attributes of the GradientBoostingRegressor</a></li>
</ol><li><a class="" href="#HistGradientBoostingClassifier">HistGradientBoostingClassifier</a></li>
<ol><li><a class="" href="#Parameters-of-the-HistGradientBoostingClassifier">Parameters of the HistGradientBoostingClassifier</a></li>
<li><a class="" href="#Attributes-of-the-HistGradientBoostingClassifier">Attributes of the HistGradientBoostingClassifier</a></li>
</ol><li><a class="" href="#HistGradientBoostingRegressor">HistGradientBoostingRegressor</a></li>
<ol><li><a class="" href="#Parameters-of-the-HistGradientBoostingRegressor">Parameters of the HistGradientBoostingRegressor</a></li>
<li><a class="" href="#Attributes-of-the-HistGradientBoostingRegressor">Attributes of the HistGradientBoostingRegressor</a></li>
</ol>
</ol>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="ensemble-learning">Ensemble Learning</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</p>
<p>Two families of ensemble methods are usually distinguished:</p>
<ul>
<li>
<p>In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</p>
<p>Examples: Bagging methods, Forests of randomized trees etc.</p>
</li>
<li>
<p>By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</p>
<p>Examples: AdaBoost, Gradient Tree Boosting</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="average-ensemble-methods">Average Ensemble Methods</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="bagging-meta-estimator">Bagging meta-estimator</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:</p>
<ul>
<li>
<p>When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as <strong>Pasting</strong>.</p>
</li>
<li>
<p>When samples are drawn with replacement, then the method is known as <strong>Bagging</strong>.</p>
</li>
<li>
<p>When random subsets of the dataset are drawn as random subsets of the features, then the method is known as <strong>Random Subspaces</strong>.</p>
</li>
<li>
<p>Finally, when base estimators are built on subsets of both samples and features, then the method is known as <strong>Random Patches</strong>.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In scikit-learn, bagging methods are offered as a unified <code>BaggingClassifier</code> meta-estimator (resp. <code>BaggingRegressor</code>), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, <code>max_samples</code> and <code>max_features</code> control the size of the subsets (in terms of samples and features), while <code>bootstrap</code> and <code>bootstrap_features</code> control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting <code>oob_score=True</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span>
                            <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For details see the papers:</p>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8562&amp;rep=rep1&amp;type=pdf">1</a></p>
<p><a href="http://www.cs.utsa.edu/~bylander/cs6243/breiman96bagging.pdf">2</a></p>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=709601">3</a></p>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.708.3190&amp;rep=rep1&amp;type=pdf">4</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="forests-of-randomized-trees">Forests of randomized trees</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>sklearn.ensemble</code> module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques specifically designed for trees. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="random-forest">Random Forest</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size <code>max_features</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The purpose of these two sources of randomness is to decrease the variance of the forest estimator. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias.</p>
<p>For details, see the <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">paper</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In scikit-learn, the <code>RandomForestClassifier</code> and <code>RandomForestRegressor</code> estimators are available.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="extremely-randomized-trees">Extremely Randomized Trees</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>0.9823000000000001</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>0.9997</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>1.0</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters">Parameters</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The main parameters to adjust when using these methods is <code>n_estimators</code> and <code>max_features</code>. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are <code>max_features=1.0</code>or equivalently <code>max_features=None</code> (always considering all features instead of a random subset) for regression problems, and <code>max_features="sqrt"</code> (using a random subset of size <code>sqrt(n_features)</code>) for classification tasks (where n_features is the number of features in the data). The default value of <code>max_features=1.0</code> is equivalent to bagged trees and more randomness can be achieved by setting smaller values.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Good results are often achieved when setting <code>max_depth=None</code> in combination with <code>min_samples_split=2</code> (i.e., when fully developing the trees). When using bootstrap sampling the generalization error can be estimated on the left out or out-of-bag samples. This can be enabled by setting <code>oob_score=True</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="time-complexity">Time Complexity</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The size of the model with the default parameters is <span class="arithmatex">\(O( M * N * log (N) )\)</span>, where <span class="arithmatex">\(M\)</span> is the number of trees and <span class="arithmatex">\(N\)</span> is the number of samples.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parallelization">Parallelization</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If <code>n_jobs=k</code> then computations are partitioned into <code>k</code> jobs, and run on <code>k</code> cores of the machine. If <code>n_jobs=-1</code> then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using <code>k</code> jobs will unfortunately not be <code>k</code> times as fast). </p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="feature-importance">Feature Importance</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In practice those estimates are stored as an attribute named <code>feature_importances_</code> on the fitted model. This is an array with shape <code>(n_features,)</code> whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.</p>
<p>You can have a look at this <a href="https://arxiv.org/pdf/1407.7502.pdf">paper</a> for more details.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="voting-classifier">Voting Classifier</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The idea behind the <code>VotingClassifier</code> is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="majority-class-labels-hard-voting">Majority Class Labels (Hard Voting)</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. This can be done by using <code>voting='hard'</code> in the <code>VotingClassifier</code> constructor.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>

<span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">clf1</span><span class="p">,</span> <span class="n">clf2</span><span class="p">,</span> <span class="n">clf3</span><span class="p">,</span> <span class="n">eclf</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s1">&#39;naive Bayes&#39;</span><span class="p">,</span> <span class="s1">&#39;Ensemble&#39;</span><span class="p">]):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">) [</span><span class="si">%s</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">label</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
Accuracy: 0.94 (+/- 0.04) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.04) [Ensemble]
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="weighted-average-probabilities-soft-voting">Weighted Average Probabilities (Soft Voting)</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</p>
<p>Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="c1"># Loading some example data</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Training classifiers</span>
<span class="n">clf1</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">clf2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">clf3</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
                        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">clf1</span><span class="p">,</span> <span class="n">clf2</span><span class="p">,</span> <span class="n">clf3</span><span class="p">,</span> <span class="n">eclf</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span> <span class="s1">&#39;Neighnours&#39;</span><span class="p">,</span> <span class="s1">&#39;SVM&#39;</span><span class="p">,</span> <span class="s1">&#39;Ensemble&#39;</span><span class="p">]):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">) [</span><span class="si">%s</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">label</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>Accuracy: 0.95 (+/- 0.03) [Decision Tree]
Accuracy: 0.94 (+/- 0.04) [Neighnours]
Accuracy: 0.95 (+/- 0.03) [SVM]
Accuracy: 0.93 (+/- 0.07) [Ensemble]
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="using-the-votingclassifier-with-gridsearchcv">Using the <code>VotingClassifier</code> with <code>GridSearchCV</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>VotingClassifier</code> can also be used together with <code>GridSearchCV</code> in order to tune the hyperparameters of the individual estimators:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span>
<span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span> <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">]}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">eclf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="voting-regressor">Voting Regressor</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The idea behind the <code>VotingRegressor</code> is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingRegressor</span>

<span class="c1"># Loading some example data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Training classifiers</span>
<span class="n">reg1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">reg2</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">reg3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">ereg</span> <span class="o">=</span> <span class="n">VotingRegressor</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;gb&#39;</span><span class="p">,</span> <span class="n">reg1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">reg2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">reg3</span><span class="p">)])</span>
<span class="n">ereg</span> <span class="o">=</span> <span class="n">ereg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="" src="https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_regressor_001.png" /></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="stacked-generalization">Stacked generalization</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Stacked generalization is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>StackingClassifier</code> and <code>StackingRegressor</code> provide such strategies which can be applied to classification and regression problems.</p>
<p>The <code>estimators</code> parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
              <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
              <span class="p">(</span><span class="s1">&#39;knr&#39;</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                                          <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))]</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>final_estimator</code> will use the predictions of the estimators as input. It needs to be a classifier or a regressor when using <code>StackingClassifier</code> or <code>StackingRegressor</code>, respectively:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingRegressor</span>
<span class="n">final_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span>
    <span class="n">final_estimator</span><span class="o">=</span><span class="n">final_estimator</span><span class="p">)</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_html rendered_html output_subarea output_execute_result">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),
                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),
                              (&#x27;knr&#x27;,
                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,
                                                   n_neighbors=20))],
                  final_estimator=GradientBoostingRegressor(max_features=1,
                                                            min_samples_leaf=25,
                                                            n_estimators=25,
                                                            random_state=42,
                                                            subsample=0.5))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div>
<p><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" ><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">StackingRegressor</label><div class="sk-toggleable__content"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),
                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),
                              (&#x27;knr&#x27;,
                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,
                                                   n_neighbors=20))],
                  final_estimator=GradientBoostingRegressor(max_features=1,
                                                            min_samples_leaf=25,
                                                            n_estimators=25,
                                                            random_state=42,
                                                            subsample=0.5))</pre></p>
</div>
</div>
</div>
<p><div class="sk-serial"><div class="sk-item"><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>ridge</label></p>
</div>
</div>
<div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">RidgeCV</label><div class="sk-toggleable__content"><pre>RidgeCV()</pre></div></div></div></div>
<p></div></div></p>
<div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>lasso</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">LassoCV</label><div class="sk-toggleable__content"><pre>LassoCV(random_state=42)</pre></div></div></div></div></div></div>
<div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>knr</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;, n_neighbors=20)</pre></div></div></div></div></div></div>
<p></div></div></p>
<div class="sk-item"><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>final_estimator</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" ><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">GradientBoostingRegressor</label><div class="sk-toggleable__content"><pre>GradientBoostingRegressor(max_features=1, min_samples_leaf=25, n_estimators=25,
                          random_state=42, subsample=0.5)</pre></div></div></div></div></div></div></div></div>
<p></div></div></div></div>
</div></p>
</div>
<p></div>
</div></p>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>During training, the estimators are fitted on the whole training data <code>X_train</code>. They will be used when calling predict or <code>predict_proba</code>. To generalize and avoid over-fitting, the <code>final_estimator</code> is trained on out-samples using sklearn.<code>model_selection.cross_val_predict</code> internally.</p>
<p>For <code>StackingClassifier</code>, note that the output of the estimators is controlled by the parameter <code>stack_method</code> and it is called by each estimator. This parameter is either a string, being estimator method names, or <code>'auto'</code> which will automatically identify an available method depending on the availability, tested in the order of preference: <code>predict_proba</code>, <code>decision_function</code> and <code>predict</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that it is also possible to get the output of the stacked <code>estimators</code> using the <code>transform</code> method:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">reg</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>array([[142.36209608, 138.30724927, 146.1       ],
       [179.700576  , 182.89812552, 151.75      ],
       [139.89817956, 132.46803343, 158.25      ],
       [286.95180286, 292.65695767, 225.4       ],
       [126.88317154, 124.1215975 , 164.65      ]])</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>In practice, a stacking predictor predicts as good as the best predictor of the base layer and even sometimes outperforms it by combining the different strengths of the these predictors. However, training a stacking predictor is computationally expensive.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Multiple stacking layers can be achieved by assigning <code>final_estimator</code> to a <code>StackingClassifier</code> or <code>StackingRegressor</code>:</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">final_layer_rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">final_layer_gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">final_layer</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">final_layer_rfr</span><span class="p">),</span>
                <span class="p">(</span><span class="s1">&#39;gbrt&#39;</span><span class="p">,</span> <span class="n">final_layer_gbr</span><span class="p">)],</span>
    <span class="n">final_estimator</span><span class="o">=</span><span class="n">RidgeCV</span><span class="p">()</span>
    <span class="p">)</span>
<span class="n">multi_layer_regressor</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
                <span class="p">(</span><span class="s1">&#39;knr&#39;</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))],</span>
    <span class="n">final_estimator</span><span class="o">=</span><span class="n">final_layer</span>
<span class="p">)</span>
<span class="n">multi_layer_regressor</span> 
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_html rendered_html output_subarea output_execute_result">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),
                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),
                              (&#x27;knr&#x27;,
                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,
                                                   n_neighbors=20))],
                  final_estimator=StackingRegressor(estimators=[(&#x27;rf&#x27;,
                                                                 RandomForestRegressor(max_features=1,
                                                                                       max_leaf_nodes=5,
                                                                                       n_estimators=10,
                                                                                       random_state=42)),
                                                                (&#x27;gbrt&#x27;,
                                                                 GradientBoostingRegressor(max_features=1,
                                                                                           max_leaf_nodes=5,
                                                                                           n_estimators=10,
                                                                                           random_state=42))],
                                                    final_estimator=RidgeCV()))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div>
<p><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">StackingRegressor</label><div class="sk-toggleable__content"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),
                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),
                              (&#x27;knr&#x27;,
                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,
                                                   n_neighbors=20))],
                  final_estimator=StackingRegressor(estimators=[(&#x27;rf&#x27;,
                                                                 RandomForestRegressor(max_features=1,
                                                                                       max_leaf_nodes=5,
                                                                                       n_estimators=10,
                                                                                       random_state=42)),
                                                                (&#x27;gbrt&#x27;,
                                                                 GradientBoostingRegressor(max_features=1,
                                                                                           max_leaf_nodes=5,
                                                                                           n_estimators=10,
                                                                                           random_state=42))],
                                                    final_estimator=RidgeCV()))</pre></p>
</div>
</div>
</div>
<p><div class="sk-serial"><div class="sk-item"><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>ridge</label></p>
</div>
</div>
<div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">RidgeCV</label><div class="sk-toggleable__content"><pre>RidgeCV()</pre></div></div></div></div>
<p></div></div></p>
<div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>lasso</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">LassoCV</label><div class="sk-toggleable__content"><pre>LassoCV(random_state=42)</pre></div></div></div></div></div></div>
<div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>knr</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;, n_neighbors=20)</pre></div></div></div></div></div></div>
<p></div></div></p>
<div class="sk-item"><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>final_estimator</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>rf</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(max_features=1, max_leaf_nodes=5, n_estimators=10,
                      random_state=42)</pre></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>gbrt</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" ><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">GradientBoostingRegressor</label><div class="sk-toggleable__content"><pre>GradientBoostingRegressor(max_features=1, max_leaf_nodes=5, n_estimators=10,
                          random_state=42)</pre></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><label>final_estimator</label></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" ><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">RidgeCV</label><div class="sk-toggleable__content"><pre>RidgeCV()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>
<p></div></div></div></div>
</div></p>
</div>
<p></div>
</div></p>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">multi_layer_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 score: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">multi_layer_regressor</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>R2 score: 0.53
</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="boosting-methods">Boosting Methods</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="adaboost">AdaBoost</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The module sklearn.ensemble includes the popular boosting algorithm <code>AdaBoost</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights <span class="arithmatex">\(w_1, w_2, \ldots, w_N\)</span>  to each of the training samples. Initially, those weights are all set to <span class="arithmatex">\(1/N\)</span>, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="" src="https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_001.png" /></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>AdaBoost can be used both for classification and regression problems:</p>
<ul>
<li>
<p>For multi-class classification, <code>AdaBoostClassifier</code> implements AdaBoost-SAMME and AdaBoost-SAMME.R.
    The paper can be found <a href="https://hastie.su.domains/Papers/SII-2-3-A8-Zhu.pdf">here</a>.</p>
</li>
<li>
<p>For regression, <code>AdaBoostRegressor</code> implements AdaBoost.R2.
    The paper can be found <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&amp;rep=rep1&amp;type=pdf">here</a>.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The number of weak learners is controlled by the parameter <code>n_estimators</code>. The <code>learning_rate</code> parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the <code>base_estimator</code> parameter. The main parameters to tune to obtain good results are <code>n_estimators</code> and the complexity of the base estimators (e.g., its depth <code>max_depth</code> or minimum required number of samples to consider a split <code>min_samples_split</code>).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="gradient-tree-boosting">Gradient Tree Boosting</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss function. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The module <code>sklearn.ensemble</code> provides methods for both classification and regression via gradient boosted decision trees.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The number of weak learners (i.e. regression trees) is controlled by the parameter <code>n_estimators</code>; The size of each tree can be controlled either by setting the tree depth via <code>max_depth</code> or by setting the number of leaf nodes via <code>max_leaf_nodes</code>. The <code>learning_rate</code> is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="n">est</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>5.009154859960321</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Both <code>GradientBoostingRegressor</code> and <code>GradientBoostingClassifier</code> support <code>warm_start=True</code> which allows you to add more estimators to an already fitted model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># set warm_start and new nr of trees</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># fit additional 100 trees to est</span>
<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>3.840234741105356</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="controlling-the-tree-size">Controlling the Tree Size</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The size of the trees can be controlled either by setting the tree depth via <code>max_depth</code> or by setting the number of leaf nodes via <code>max_leaf_nodes</code>.</p>
<p>If you specify <code>max_depth=h</code> then complete binary trees of depth <code>h</code> will be grown. Such trees will have (at most) <code>2**h</code> leaf nodes and <code>2**h - 1</code> split nodes.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter <code>max_leaf_nodes</code>. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with <code>max_leaf_nodes=k</code> has k<code>- 1</code> split nodes and thus can model interactions of up to order <code>max_leaf_nodes - 1</code> .</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Usually <code>max_leaf_nodes=k</code> gives comparable results to <code>max_depth=k-1</code> but is significantly faster to train at the expense of a slightly higher training error.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="loss-functions">Loss Functions</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following loss functions are supported and can be specified using the parameter <code>loss</code>:</p>
<ol>
<li>
<p>Regression</p>
<ul>
<li>
<p>Squared error (<code>'squared_error'</code>): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</p>
</li>
<li>
<p>Least absolute deviation (<code>'lad'</code>): A robust loss function for regression. The initial model is given by the median of the target values.</p>
</li>
<li>
<p>Huber (<code>'huber'</code>): Another robust loss function that combines least squares and least absolute deviation; use <code>alpha</code> to control the sensitivity with regards to outliers.</p>
</li>
<li>
<p>Quantile (<code>'quantile'</code>): A loss function for quantile regression. Use <code>0 &lt; alpha &lt; 1</code> to specify the quantile. This loss function can be used to create prediction intervals</p>
</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>
<p>Classification</p>
<ul>
<li>
<p>Binary log-loss (<code>'log-loss'</code>): The binomial negative log-likelihood loss function for binary classification. It provides probability estimates. The initial model is given by the log odds-ratio.</p>
</li>
<li>
<p>Multi-class log-loss (<code>'log-loss'</code>): The multinomial negative log-likelihood loss function for multi-class classification with <code>n_classes</code> mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration <code>n_classes</code> regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</p>
</li>
<li>
<p>Exponential loss (<code>'exponential'</code>): The same loss function as <code>AdaBoostClassifier</code>. Less robust to mislabeled examples than <code>'log-loss'</code>; can only be used for binary classification.</p>
</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="histogram-based-gradient-boosting">Histogram-Based Gradient Boosting</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Scikit-learn provides a histogram-based gradient boosting method called <code>HistGradientBoostingRegressor</code> or <code>HistGradientBoostingClassifier</code> based on LightGBM.</p>
<p>These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples.</p>
<p>They also have built-in support for missing values, which avoids the need for an imputer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These fast estimators first bin the input samples X into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from <code>GradientBoostingClassifier</code> and <code>GradientBoostingRegressor</code> are not yet supported, for instance some loss functions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>n_estimators</code> parameters is changed with <code>max_iter</code> while most of the other parameters are the same as <code>GradientBoostingClassifier</code> and <code>GradientBoostingRegressor</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>0.8965</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>0.8715</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Available losses for regression are squared_error, absolute_error, which is less sensitive to outliers, and poisson, which is well suited to model counts and frequencies. For classification, log_loss is the only option. For binary classification it uses the binary log loss, also kown as binomial deviance or binary cross-entropy. For <code>n_classes &gt;= 3</code>, it uses the multi-class log loss function, with multinomial deviance and categorical cross-entropy as alternative names. The appropriate loss version is selected based on <code>y</code> passed to <code>fit</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The size of the trees can be controlled through the <code>max_leaf_nodes</code>, <code>max_depth</code>, and <code>min_samples_leaf</code> parameters.</p>
<p>The number of bins used to bin the data is controlled with the <code>max_bins</code> parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="missing-values-support">Missing values support</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>HistGradientBoostingClassifier</code> and <code>HistGradientBoostingRegressor</code> have built-in support for missing values (NaNs).</p>
<p>During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                      <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">gbdt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>array([0, 1, 0, 0, 1])</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="sample-weight-support">Sample weight support</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>HistGradientBoostingClassifier</code> and <code>HistGradientBoostingRegressor</code> sample support weights during fit.</p>
<p>The following toy example demonstrates how the model ignores the samples with zero sample weights:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="c1"># ignore the first 2 training samples by setting their weight to 0</span>
<span class="n">sample_weight</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">gb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">gb</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>

<span class="n">gb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div>

</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>
<code>[1]
</code>
</pre>
</div>
</div>
<div class="output_area">
<div class="output_text output_subarea output_execute_result">
<pre>
<code>0.9990209190235209</code>
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, the <code>[1, 0]</code> is comfortably classified as 1 since the first two samples are ignored due to their sample weights.</p>
<blockquote>
<p>Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="monotonic-constraints">Monotonic Constraints</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Depending on the problem at hand, you may have prior knowledge indicating that a given feature should in general have a positive (or negative) effect on the target value. For example, all else being equal, a higher credit score should increase the probability of getting approved for a loan. Monotonic constraints allow you to incorporate such prior knowledge into the model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A positive monotonic constraint is a constraint of the form:
<span class="arithmatex">\(<span class="arithmatex">\(x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)\)</span>\)</span>
where  is the predictor with two features.</p>
<p>Similarly, a negative monotonic constraint is of the form:
<span class="arithmatex">\(<span class="arithmatex">\(x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)\)</span>\)</span></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can specify a monotonic constraint on each feature using the <code>monotonic_cst</code> parameter. For each feature, a value of 0 indicates no constraint, while -1 and 1 indicate a negative and positive constraint, respectively:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="c1"># positive, negative, and no constraint on the 3 features</span>
<span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">monotonic_cst</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</code></pre></div>

</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>In a binary classification context, imposing a monotonic constraint means that the feature is supposed to have a positive / negative effect on the probability to belong to the positive class. Monotonic constraints are not supported for multiclass context.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="algorithms">Algorithms</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="baggingclassifier"><code>BaggingClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-baggingclassifier">Parameters of the <code>BaggingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>BaggingClassifier</code>:
* <strong>base_estimator</strong>: object, default=None</p>
<div class="highlight"><pre><span></span><code>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a `DecisionTreeClassifier`.
</code></pre></div>
<ul>
<li>
<p><strong>n_estimators</strong>: int, default=10</p>
<p>The number of base estimators in the ensemble.
* <strong>max_samples</strong>: int or float, default=1.0</p>
<p>The number of samples to draw from X to train each base estimator (with replacement by default).</p>
<ul>
<li>
<p>If int, then draw <code>max_samples</code> samples.</p>
</li>
<li>
<p>If float, then draw <code>max_samples * X.shape[0]</code> samples.</p>
</li>
<li><strong>max_features</strong>: int or float, default=1.0</li>
</ul>
<p>The number of features to draw from X to train each base estimator (with replacement by default).</p>
<ul>
<li>
<p>If int, then draw <code>max_features</code> features.</p>
</li>
<li>
<p>If float, then draw <code>max_features * X.shape[1]</code> features.</p>
</li>
<li><strong>bootstrap</strong>: boolean, default=True</li>
</ul>
<p>If True, bootstrap samples are used to train each base estimator.
* <strong>bootstrap_features</strong>: boolean, default=False</p>
<p>If True, bootstrap samples are used to train each base estimator.
* <strong>oob_score</strong>: boolean, default=False</p>
<p>If True, the base estimators are trained on the out-of-bag samples. Only available if <code>bootstrap=True</code>.
* <strong>warm_start</strong>: boolean, default=False</p>
<p>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-baggingclassifier">Attributes of the <code>BaggingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>BaggingClassifier</code>:
* <strong>estimators_</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>The collection of fitted base estimators.
</code></pre></div>
<ul>
<li>
<p><strong>base_estimator_</strong>: estimator</p>
<p>The base estimator from which the ensemble is grown.
* <strong>n_features_</strong>: int</p>
<p>The number of features when <code>fit</code> is performed.
* <strong>n_classes_</strong>: int</p>
<p>The number of classes when <code>fit</code> is performed.
* <strong>classes_</strong>: ndarray of shape (n_classes,)</p>
<p>The classes labels.
* <strong>estimators_samples_</strong>: array-like of shape [n_estimators, n_samples]</p>
<p>The subset of drawn samples (i.e., the training samples) for each base estimator.
* <strong>estimators_features_</strong>: array-like of shape [n_estimators, n_features]</p>
<p>The subset of drawn features for each base estimator.
* <strong>oob_score_</strong>: float</p>
<p>Score of the training dataset obtained using an out-of-bag estimate.    </p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="baggingregressor"><code>BaggingRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-baggingregressor">Parameters of the <code>BaggingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>BaggingClassifier</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-baggingregressor">Attributes of the <code>BaggingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>BaggingClassifier</code> but excluding the classes related attributes.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="randomforestclassifier"><code>RandomForestClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-randomforestclassifier">Parameters of the <code>RandomForestClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>RandomForestClassifier</code>:
* <strong>n_estimators</strong>: int, default=10</p>
<div class="highlight"><pre><span></span><code>The number of trees in the forest.
</code></pre></div>
<ul>
<li>
<p><strong>criterion</strong>: {gini, entropy, log_loss}, default=gini</p>
<p>The function to measure the quality of a split. Supported criteria are "gini" for the Gini impurity and "entropy" for the information gain.
* <strong>max_depth</strong>: int or None, default=None</p>
<p>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
* <strong>max_leaf_nodes</strong>: int or None, default=None</p>
<p>Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
* <strong>splitter</strong>: {best, random}, default=best</p>
<p>The strategy used to choose the best split. Valid options are best to choose the best split and random to choose at random.
* <strong>min_samples_split</strong>: int, default=2</p>
<p>The minimum number of samples required to split an internal node.
- If int, then consider <code>min_samples_split</code> as the minimum number.</p>
<ul>
<li>If float, then <code>min_samples_split</code> is a fraction and <code>ceil(min_samples_split * n_samples)</code> are the minimum number of samples for each split.</li>
<li><strong>min_samples_leaf</strong>: int, default=1</li>
</ul>
<p>The minimum number of samples required to be at a leaf node.
- If int, then consider <code>min_samples_leaf</code> as the minimum number.</p>
<ul>
<li>If float, then <code>min_samples_leaf</code> is a fraction and <code>ceil(min_samples_leaf * n_samples)</code> are the minimum number of samples for each node.</li>
<li><strong>min_weight_fraction_leaf</strong>: float, default=0.0</li>
</ul>
<p>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.
* <strong>max_features</strong>: sqrt, log2, None}, int or float, default=sqrt</p>
<p>The number of features to consider when looking for the best split:
- If int, then consider <code>max_features</code> features at each split.
- If float, then <code>max_features</code> is a fraction and <code>int(max_features * n_features)</code> features are considered at each split.
- If auto, then <code>max_features=sqrt(n_features)</code>.
- If sqrt, then <code>max_features=sqrt(n_features)</code>.
- If log2, then <code>max_features=log2(n_features)</code>.
- If None, then <code>max_features=n_features</code>.
* <strong>max_leaf_nodes</strong>: int or None, default=None</p>
<p>Grow trees with <code>max_leaf_nodes</code> in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
* <strong>bootstrap</strong>: boolean, default=True</p>
<p>Whether bootstrap samples are used when building trees.
* <strong>oob_score</strong>: boolean, default=False</p>
<p>Whether to use out-of-bag samples to estimate the generalization accuracy. Only available if bootstrap=True.
* <strong>warm_start</strong>: boolean, default=False</p>
<p>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.
* <strong>class_weight</strong>: dict, list of dicts, balanced, or None, default=None</p>
<p>Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.
- For multi-output, the weights are assigned to the first output of each base learner.
- If balanced, then classes are reweighted such that they sum to n_samples.
- If a dict or list of dicts, then the weights are assigned to the keys.
- If None, then all classes are supposed to have weight one.
* <strong>max_sample_count</strong>: int, default=None</p>
<p>If bootstrap is True, the number of samples to draw from X to train each base estimator.</p>
<ul>
<li>
<p>If None (default), then draw X.shape[0] samples.</p>
</li>
<li>
<p>If int, then draw max_samples samples.</p>
</li>
</ul>
</li>
<li>
<p>If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0.0, 1.0].</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-randomforestclassifier">Attributes of the <code>RandomForestClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>RandomForestClassifier</code>:
* <strong>n_estimators_</strong>: int</p>
<div class="highlight"><pre><span></span><code>The number of trees in the forest.
</code></pre></div>
<ul>
<li>
<p><strong>estimators_</strong>: list of estimators</p>
<p>The collection of fitted base estimators.
* <strong>classes_</strong>: ndarray of shape (n_classes,)</p>
<p>The classes labels.
* <strong>base_estimator_</strong>: estimator</p>
<p>The base estimator from which the ensemble is grown.
* <strong>oob_score_</strong>: float</p>
<p>Score of the training dataset obtained using an out-of-bag estimate.
* <strong>feature_importances_</strong>: ndarray of shape (n_features,)</p>
<p>The feature importances attribute of the forest.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="randomforestregressor"><code>RandomForestRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-randomforestregressor">Parameters of the <code>RandomForestRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>RandomForestClassifier</code> but with following exception/addition:
* <strong>criterion</strong>: {squared_error, absolute_error, poisson}, default=squared_error</p>
<div class="highlight"><pre><span></span><code>The function to measure the quality of a split. Supported criteria are squared_error for the mean squared error, which is equal to variance reduction as feature selection criterion, absolute_error for the mean absolute error, and poisson which uses reduction in Poisson deviance to find splits. Training using absolute_error is significantly slower than when using squared_error.
</code></pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-randomforestregressor">Attributes of the <code>RandomForestRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>RandomForestClassifier</code> excluding the class related attributes.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="extratreesclassifier"><code>ExtraTreesClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-extratreesclassifier">Parameters of the <code>ExtraTreesClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>RandomForestClassifier</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-extratreesclassifier">Attributes of the <code>ExtraTreesClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>RandomForestClassifier</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="extratreesregressor"><code>ExtraTreesRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-extratreesregressor">Parameters of the <code>ExtraTreesRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>RandomForestRegressor</code>. But the <code>criterion</code> parameter does not support poisson.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-extratreesregressor">Attributes of the <code>ExtraTreesRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>RandomForestRegressor</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="votingclassifier"><code>VotingClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-votingclassifier">Parameters of the <code>VotingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>VotingClassifier</code>:
* <strong>voting</strong>: {hard, soft}, default=hard</p>
<div class="highlight"><pre><span></span><code>If hard, uses predicted class labels for majority voting.
If soft, predicts the class label that is most common in the majority class subset.
</code></pre></div>
<ul>
<li>
<p><strong>estimators</strong>: tuples or list of estimator</p>
<p>The base estimators for the voting classifier. If list of tuples, the weights of the estimators are used for voting. If list of estimators, the estimators are used for voting.
* <strong>weights</strong>: array-like of shape (n_classifiers,), default=None</p>
<p>Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.
* <strong>n_jobs</strong>: int, default=1</p>
<p>The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>. If -1, then the number of jobs is set to the number of cores.
* <strong>flatten_transform</strong>: boolean, default=True</p>
<p>Affects shape of transform output only when voting=soft If voting=soft and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-votingclassifier">Attributes of the <code>VotingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>VotingClassifier</code>:
* <strong>estimators_</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>The collection of fitted base estimators.
</code></pre></div>
<ul>
<li>
<p><strong>named_estimators</strong>: dict of estimators</p>
<div class="highlight"><pre><span></span><code>A dictionary mapping the names of the estimators to the fitted estimators.
</code></pre></div>
<ul>
<li><strong>le_</strong>: LabelEncoder</li>
</ul>
<p>The LabelEncoder used to encode the class labels.
* <strong>classes_</strong>: ndarray of shape (n_classes,)</p>
<p>The classes labels.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="votingregressor"><code>VotingRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-votingregressor">Parameters of the <code>VotingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>VotingRegressor</code>:
* <strong>estimators</strong>: tuples or list of estimator</p>
<div class="highlight"><pre><span></span><code>The base estimators for the voting classifier. If list of tuples, the weights of the estimators are used for voting. If list of estimators, the estimators are used for voting.
</code></pre></div>
<ul>
<li>
<p><strong>weights</strong>: array-like of shape (n_classifiers,), default=None</p>
<p>Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.
* <strong>n_jobs</strong>: int, default=1</p>
<p>The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>. If -1, then the number of jobs is set to the number of cores.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-votingregressor">Attributes of the <code>VotingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>VotingRegressor</code>:
* <strong>estimators_</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>The collection of fitted base estimators.
</code></pre></div>
<ul>
<li><strong>named_estimators</strong>: dict of estimators<div class="highlight"><pre><span></span><code>A dictionary mapping the names of the estimators to the fitted estimators.
</code></pre></div>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="stackingclassifier"><code>StackingClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-stackingclassifier">Parameters of the <code>StackingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>StackingClassifier</code>:
* <strong>estimators</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to drop using set_params.
</code></pre></div>
<ul>
<li>
<p><strong>final_estimator</strong>: estimator, default=None</p>
<p>The final estimator to be used in the StackingClassifier.  The default classifier is a LogisticRegression.
* <strong>cv</strong>: int, cross-validation generator or an iterable, default=None</p>
<p>Determines the cross-validation splitting strategy. Possible inputs for cv are:
* None, to use the default 5-fold cross-validation,
* integer, to specify the number of folds.
* An object to be used as a cross-validation generator.
* An iterable yielding train, test splits.
* An iterable yielding (train, test) splits.
* A mapping from output names to (train, test) splits.
* A callable, which returns a training and test set.</p>
<p>If prefit is passed, it is assumed that all estimators have been fitted already. The final_estimator_ is trained on the estimators predictions on the full training set and are not cross validated predictions. Please note that if the models have been trained on the same data to train the stacking model, there is a very high risk of overfitting.
* <strong>stack_method</strong>: {auto, predict_proba, decision_function, predict}, default=auto</p>
<p>The method used to combine the predictions of the base estimators. If predict, the mean of the predicted values of the base estimators is used. If predict_proba, the mean of the predicted probabilities of the base estimators is used. If decision_function, the mean of the predicted decision function of the base estimators is used.
* <strong>passthrough</strong>: boolean, default=False</p>
<p>When False, only the predictions of estimators will be used as training data for final_estimator. When True, the final_estimator is trained on the predictions as well as the original training data.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-stackingclassifier">Attributes of the <code>StackingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>StackingClassifier</code>:
* <strong>estimators_</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>The collection of fitted base estimators.
</code></pre></div>
<ul>
<li>
<p><strong>named_estimators</strong>: dict of estimators</p>
<div class="highlight"><pre><span></span><code>A dictionary mapping the names of the estimators to the fitted estimators.
</code></pre></div>
<ul>
<li><strong>classes_</strong>: ndarray of shape (n_classes,)</li>
</ul>
<p>The classes labels.
* <strong>final_estimator_</strong>: estimator</p>
<p>TThe classifier which predicts given the output of <code>estimators_</code>.
* <strong>stack_method_</strong>: list ot str</p>
<p>The method used by each base estimator.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="stackingregressor"><code>StackingRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-stackingregressor">Parameters of the <code>StackingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>StackingClassifier</code> except for the <code>stack_method</code> parameter.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-stackingregressor">Attributes of the <code>StackingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>StackingClassifier</code> except for the class related attributes.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="adaboostclassifier"><code>AdaBoostClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-adaboostclassifier">Parameters of the <code>AdaBoostClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>AdaBoostClassifier</code>:
* <strong>base_estimator</strong>: estimator, default=None</p>
<div class="highlight"><pre><span></span><code>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.
</code></pre></div>
<ul>
<li>
<p><strong>n_estimators</strong>: int, default=50</p>
<p>The maximum number of estimators at which boosting is terminated.
* <strong>learning_rate</strong>: float, default=1.</p>
<p>Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier.of each classifier by <code>learning_rate</code>. There is a trade-off between <code>learning_rate</code> and <code>n_estimators</code>.
* <strong>algorithm</strong>: {SAMME, SAMME.R}, default=SAMME.R</p>
<p>If SAMME.R then use the SAMME.R real boosting algorithm. <code>base_estimator</code> must support calculation of class probabilities. If SAMME then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-adaboostclassifier">Attributes of the <code>AdaBoostClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>AdaBoostClassifier</code>:
* <strong>estimators_</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>The collection of fitted base estimators.
</code></pre></div>
<ul>
<li>
<p><strong>base_estimator_</strong>: estimator</p>
<p>The base estimator from which the boosted ensemble is built.
* <strong>classes_</strong>: ndarray of shape (n_classes,)</p>
<p>The classes labels.
* <strong>n_classes_</strong>: int</p>
<p>The number of classes.
* <strong>estimator_weights_</strong>: ndarray of shape (n_estimators,)</p>
<p>The weights for each estimator in the boosted ensemble.
* <strong>estimator_errors_</strong>: ndarray of shape (n_estimators,)</p>
<p>The classifier loss functions for each estimator in the boosted ensemble.
* <strong>feature_importances_</strong>: ndarray of shape (n_features,)</p>
</li>
</ul>
<p>The impurity-based feature importances.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="adaboostregressor"><code>AdaBoostRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-adaboostregressor">Parameters of the <code>AdaBoostRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>AdaBoostClassifier</code> except for the <code>loss</code> parameter.
* <strong>loss</strong>: {linear, square, exponential}, default=linear</p>
<div class="highlight"><pre><span></span><code>The loss function to be used. linear, square, and exponential perform linear, squared, and exponential loss, respectively.
</code></pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-adaboostregressor">Attributes of the <code>AdaBoostRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>AdaBoostClassifier</code> except for the class related attributes.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="gradientboostingclassifier"><code>GradientBoostingClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the parameters of the <code>GradientBoostingClassifier</code>:
* <strong>loss</strong>:{log_loss, deviance, exponential}, default=log_loss</p>
<div class="highlight"><pre><span></span><code>The loss function to be optimized. log_loss refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss exponential, gradient boosting recovers the AdaBoost algorithm.
</code></pre></div>
<ul>
<li>
<p><strong>learning_rate</strong>: float, default=0.1</p>
<p>Learning rate shrinks the contribution of each tree by <code>learning_rate</code>. There is a trade-off between <code>learning_rate</code> and <code>n_estimators</code>.
* <strong>n_estimators</strong>: int, default=100</p>
<p>The maximum number of estimators at which boosting is terminated.
* <strong>subsample</strong>: float, default=1.0</p>
<p>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0, the model will randomly sample this fraction of samples when fitting each base learters. If greater than 1.0, it will use the entire dataset for fitting each base learters.
* <strong>criterion</strong>: {friedman_mse, mse, mae}, default=friedman_mse</p>
<p>The function to measure the quality of a split. Supported criteria are friedman_mse, mse, and mae. The best split is the one that minimizes the criterion function.
* <strong>init</strong>: estimator or zero, default=None</p>
</li>
</ul>
<p>An estimator object that is used to compute the initial predictions. <code>init</code> has to provide <code>fit</code> and <code>predict_proba</code>. If zero, the initial raw predictions are set to zero. By default, a <code>DummyEstimator</code> predicting the classes priors is used.
* <strong>validation_fraction</strong>: float, default=0.1</p>
<div class="highlight"><pre><span></span><code>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if validation_fraction &gt; 0.
</code></pre></div>
<p><strong>Plus other parameters of the <code>RandomForestClassifier</code> like <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code>, <code>min_weight_fraction_leaf</code>, <code>max_features</code>, <code>max_leaf_nodes</code>, <code>min_impurity_decrease</code>, <code>min_impurity_split</code>, <code>bootstrap</code>, <code>oob_score</code>, <code>n_jobs</code>, <code>random_state</code>, <code>verbose</code>, <code>warm_start</code>, <code>class_weight</code></strong></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-gradientboostingclassifier">Attributes of the <code>GradientBoostingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>GradientBoostingClassifier</code>:
* <strong>estimators_</strong>: list of estimators</p>
<div class="highlight"><pre><span></span><code>The collection of fitted base estimators.
</code></pre></div>
<ul>
<li>
<p><strong>n_estimators_</strong>: int</p>
<p>The number of estimators in the fitted ensemble.
* <strong>feature_importances_</strong>: ndarray of shape (n_features,)</p>
<p>The impurity-based feature importances.
* <strong>oob_improvement_</strong>: ndarray of shape (n_estimators,)</p>
<p>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. <code>oob_improvement_[0]</code> is the improvement in loss of the first stage over the <code>init</code> estimator. Only available if <code>subsample &lt; 1.0</code>
* <strong>train_score_</strong>: ndarray of shape (n_estimators,)</p>
<p>The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample == 1 this is the deviance on the training data.
* <strong>classes_</strong>: ndarray of shape (n_classes,)</p>
<p>The classes labels.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="gradientboostingregressor"><code>GradientBoostingRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-gradientboostingregressor">Parameters of the <code>GradientBoostingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>GradientBoostingClassifier</code> except for the <code>loss</code> parameter.
* <strong>loss</strong>: squared_error, absolute_error, huber, quantile}, default=squared_error</p>
<div class="highlight"><pre><span></span><code>Loss function to be optimized. squared_error refers to the squared error for regression. absolute_error refers to the absolute error of regression and is a robust loss function. huber is a combination of the two. quantile allows quantile regression (use `alpha` to specify the quantile).
</code></pre></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-gradientboostingregressor">Attributes of the <code>GradientBoostingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>GradientBoostingClassifier</code> except for the class related attributes.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="histgradientboostingclassifier"><code>HistGradientBoostingClassifier</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-histgradientboostingclassifier">Parameters of the <code>HistGradientBoostingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>HHere are some of the parameters of the <code>HistGradientBoostingClassifier</code>:
* <strong>loss</strong>: {log_loss, auto, binary_crossentropy, categorical_crossentropy}, default=log_loss</p>
<div class="highlight"><pre><span></span><code> The loss function to use in the boosting process.

For binary classification problems, log_loss is also known as logistic loss, binomial deviance or binary crossentropy. Internally, the model fits one tree per boosting iteration and uses the logistic sigmoid function (expit) as inverse link function to compute the predicted positive class probability.

For multiclass classification problems, log_loss is also known as multinomial deviance or categorical crossentropy. Internally, the model fits one tree per boosting iteration and per class and uses the softmax function as inverse link function to compute the predicted probabilities of the classes.
</code></pre></div>
<ul>
<li>
<p><strong>learning_rate</strong>: float, default=0.1</p>
<p>The learning rate, also known as shrinkage. This is used as a multiplicative factor for the leaves values. Use 1 for no shrinkage.
* <strong>max_iter</strong>: int, default=100</p>
<p>The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, n_classes trees per iteration are built.
* <strong>l2_regularization</strong>: float, default=0.0</p>
<p>L2 regularization parameter. The larger this is, the more regularization, the less regularization, the more sparsity.
* <strong>monotonic_cst</strong>: array-like of int of shape (n_features), default=Nonearray-like of int of shape (n_features), default=None</p>
<p>Indicates the monotonic constraint to enforce on each feature. -1, 1 and 0 respectively correspond to a negative constraint, positive constraint and no constraint.
* <strong>early_stopping</strong>: bool, default=False</p>
<p>If auto, early stopping is enabled if the sample size is larger than 10000. If True, early stopping is enabled, otherwise early stopping is disabled.
* <strong>validation_fraction</strong>: float, default=0.1</p>
<p>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if validation_fraction &gt; 0.
* <strong>scoring</strong>: str, default=None</p>
<p>A string or a callable to evaluate the predictions on the test set. If None, the score method of the estimator is used. If a callable is given, it is called with two arguments and must return a scalar value.</p>
</li>
</ul>
<p><strong>Plus Other parameters from the <code>GradientBoostingClassifier</code> like <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code>, <code>min_weight_fraction_leaf</code>, <code>max_features</code>, <code>max_leaf_nodes</code>, <code>min_impurity_decrease</code>, <code>min_impurity_split</code>, <code>bootstrap</code>, <code>oob_score</code>, <code>n_jobs</code>, <code>random_state</code>, <code>verbose</code>, <code>warm_start</code>, <code>class_weight</code></strong></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-histgradientboostingclassifier">Attributes of the <code>HistGradientBoostingClassifier</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some of the attributes of the <code>HistGradientBoostingClassifier</code>:
* <strong>classes_</strong>: ndarray of shape (n_classes,)</p>
<div class="highlight"><pre><span></span><code> The classes labels.
</code></pre></div>
<ul>
<li>
<p><strong>do_early_stopping_</strong>: bool</p>
<p>Whether early stopping is performed.
* <strong>n_iter_</strong>: int</p>
<p>The number of boosting iterations computed.
* <strong>train_score_</strong>: ndarray of shape (n_iter,)</p>
<p>The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the scoring parameter. If <code>scoring</code> is not loss, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.
* <strong>validation_score_</strong>: ndarray of shape (n_iter,)</p>
<p>The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the <code>scoring</code> parameter. Empty if no early stopping or if <code>validation_fraction</code> is None.
* <strong>n_trees_per_iteration_</strong>: int</p>
<p>The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to <code>n_classes</code> for multiclass classification.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="histgradientboostingregressor"><code>HistGradientBoostingRegressor</code></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameters-of-the-histgradientboostingregressor">Parameters of the <code>HistGradientBoostingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>HistGradientBoostingClassifier</code> except for the <code>loss</code> parameter.
* <strong>loss</strong>: {squared_error, absolute_error, poisson, quantile}, default=squared_error</p>
<div class="highlight"><pre><span></span><code>The loss function to use in the boosting process. Note that the squared error and poisson losses actually implement half least squares loss and half poisson deviance to simplify the computation of the gradient. Furthermore, poisson loss internally uses a log-link and requires `y &gt;= 0`. quantile uses the pinball loss.
</code></pre></div>
<ul>
<li>
<p><strong>quantile</strong>: float, default=None</p>
<p>The quantile used to compute the quantile loss. Must be between 0 and 1. Only used if loss is quantile.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="attributes-of-the-histgradientboostingregressor">Attributes of the <code>HistGradientBoostingRegressor</code></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same as <code>HistGradientBoostingClassifier</code> except for the class related attributes.</p>
</div>
</div>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/hari31416" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/hari31416" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/hari31416" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.kaggle.com/hari31416" target="_blank" rel="noopener" title="www.kaggle.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M304.2 501.5 158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.instagram.com/hari31416" target="_blank" rel="noopener" title="www.instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "navigation.top", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>